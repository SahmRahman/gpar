# -*- coding: utf-8 -*-
"""STAT0035 Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EIyv_OmfhwVD_5KTqAZvXIKv1-a_vwvZ
"""

import libraries


class WindFarmGPAR:

    def __init__(self, train_data_path, test_data_path, local_path, model_params):
        """
        initialiser for a model

        :param train_data_path: filepath, string
        :param test_data_path: filepath, string
        :param local_path: filepath, string
        :param model_params: list of 9 elements, parameters for the GPARRegressor model
        """
        self.train_data = libraries.pd.read_pickle(train_data_path)
        self.test_data = libraries.pd.read_pickle(test_data_path)
        self.local_path = local_path
        self.model_params = model_params
        # need to have some control over missing filepath

    def sample_data(self):
        train_sample = self.train_data.iloc[:100]
        test_sample = self.test_data.iloc[:100]

        return {"train": train_sample,
                "test": test_sample}

    @staticmethod
    def specify_data(df, columns):
        """
        reshapes selected inputs to be agreeable with GPARs methods

        :param df: input values, pandas DataFrame
        :param columns: input columns, list of strings
        :return: 2D ndarray of selected columns from the dataframe, numpy ndarray
        """
        return df[columns].values

    @staticmethod
    def log_results(results_df, input_cols, output_cols, training_indices, test_indices):
        """
        log the results of a model run

        :param results_df: outputs, pandas DataFrame
        :param input_cols: input columns, list of strings
        :param output_cols: output columns, list of strings
        :param training_indices: indices of the original training data dataframe, list of ints
        :param test_indices: indices of the original test data dataframe, list of ints
        :return: NONE, simply saves results to pickle file and prints the filename
        """

        # Get the current timestamp
        timestamp = libraries.datetime.now().strftime("%Y-%m-%d_%H-%M")

        # Create the dynamic filename
        filename = f"test results_{timestamp}.pkl"

        results_df['Input Columns'] = input_cols
        results_df['Output Columns'] = output_cols
        results_df['Training Data Indices'] = training_indices
        results_df['Test Data Indices'] = test_indices

        # Save the DataFrame as a Pickle file
        results_df.to_pickle(filename)

        print(f"DataFrame saved to {filename} yeet")

    def train_model(self, input_columns, output_columns):
        """
        1) sample data of requested input and output columns
        2) create the GPARRegressor model
        3) train the model
        4) store the results

        :param input_columns: input columns, list of strings
        :param output_columns: output columns, list of strings
        :return: NONE, trains model and logs the results
        """

        for cols in [input_columns, output_columns]:
            if 'index' in cols:
                cols.remove('index')

        sample_dict = self.sample_data()
        train_sample = sample_dict['train']
        test_sample = sample_dict['test']
        # establish training and testing data for GPARRegressor model
        # want to preserve these to be pandas DataFrames

        # adjust shape/data type of data sample to be ndarrays for GPAR
        training_input = WindFarmGPAR.specify_data(train_sample, input_columns)
        training_output = WindFarmGPAR.specify_data(train_sample, output_columns)
        test_input = WindFarmGPAR.specify_data(test_sample, input_columns)
        test_output = WindFarmGPAR.specify_data(test_sample, output_columns)

        # Fit and predict GPAR.
        model = libraries.gpar.GPARRegressor(self.model_params)
            # scale=0.1,  # Initial length scale for the inputs.
            # linear=True,  # Use linear dependencies between outputs.
            # linear_scale=10.0,  # Length scale for linear dependencies between outputs.
            # nonlinear=True,  # Also use nonlinear dependencies between outputs.
            # nonlinear_scale=0.1,  # Length scale for nonlinear dependencies (post-normalisation, if enabled).
            # noise=0.1,  # Variance of the observation noise.
            # impute=True,  # Impute missing data to ensure data is closed downwards.
            # replace=False,  # Do not replace data points with posterior mean of previous layer (retains noise).
            # normalise_y=False  # Work with raw outputs, without normalising them.


        model_df = libraries.pd.DataFrame(columns=['scale', 'linear', ...])
        model_df[1] = [1, False, 1000]

        dummy_model = libraries.gpar.GPARRegressor(
            scale=1,  # Initial length scale for the inputs.
            linear=False,  # Use linear dependencies between outputs.
            linear_scale=10.0,  # Length scale for linear dependencies between outputs.
            nonlinear=True,  # Also use nonlinear dependencies between outputs.
            nonlinear_scale=0.1,  # Length scale for nonlinear dependencies (post-normalisation, if enabled).
            noise=0.1,  # Variance of the observation noise.
            impute=True,  # Impute missing data to ensure data is closed downwards.
            replace=False,  # Do not replace data points with posterior mean of previous layer (retains noise).
            normalise_y=False  # Work with raw outputs, without normalising them.
        )

        dummy_df = libraries.pd.DataFrame(
            columns=['scale', 'linear', 'linear_scale', 'nonlinear', 'nonlinear_scale', 'noise', 'impute', 'replace',
                     'normalise_y'])

        dummy_df.loc[0] = [1, False, 10, True, 0.1, 0.1, True, False, False]
        print(dummy_df)

        dummy_df.to_pickle('dummy test pickle.pkl')

        # train model
        model.fit(training_input, training_output)

        # collect metadata
        means, lowers, uppers = model.predict(test_input,
                                              num_samples=5,
                                              credible_bounds=True)

        error = means - test_output

        metadata = {
            "Means": means.flatten(),
            "Lowers": lowers.flatten(),
            "Uppers": uppers.flatten(),
            "Error": error.flatten()
        }

        # organise and log results
        train_indices = train_sample[['index']].values.flatten()
        test_indices = test_sample[['index']].values.flatten()

        metadata = libraries.pd.DataFrame(metadata)

        WindFarmGPAR.log_results(
            results_df=metadata,
            input_cols=input_columns,
            output_cols=output_columns,
            training_indices=train_indices,
            test_indices=test_indices
        )


model_obj = WindFarmGPAR(train_data_path=libraries.os.getcwd() + "/Wind farm final year project _ SR_DL_PD/train.pkl",
                         test_data_path=libraries.os.getcwd() + "/Wind farm final year project _ SR_DL_PD/test.pkl",
                         local_path=libraries.os.getcwd())

model_obj.train_model(input_columns=['Wind.speed.me'],
                      output_columns=['Power.me'])

