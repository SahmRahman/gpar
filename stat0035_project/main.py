# -*- coding: utf-8 -*-
"""STAT0035 Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EIyv_OmfhwVD_5KTqAZvXIKv1-a_vwvZ
"""
import pandas as pd

import libraries


class WindFarmGPAR:

    def __init__(self, train_data_path, test_data_path, local_path,
                 replace=False,
                 impute=True,
                 scale=1.0,
                 scale_tie=False,
                 per=False,
                 per_period=1.0,
                 per_scale=1.0,
                 per_decay=10.0,
                 input_linear=False,
                 input_linear_scale=100.0,
                 linear=True,
                 linear_scale=100.0,
                 nonlinear=False,
                 nonlinear_scale=1.0,
                 rq=False,
                 markov=None,
                 noise=0.1,
                 x_ind=None,
                 normalise_y=True,
                 transform_y=None):
        self.train_data = libraries.pd.read_pickle(train_data_path)
        self.test_data = libraries.pd.read_pickle(test_data_path)
        self.local_path = local_path
        # need to have some control over missing filepath

    def sample_data(self):
        train_sample = self.train_data.iloc[:100]
        test_sample = self.test_data.iloc[:100]

        return {"train": train_sample,
                "test": test_sample}

    @staticmethod
    def specify_data(df, columns):
        return df[columns].values

    @staticmethod
    def log_results(results_df, input_cols, output_cols, training_indices, test_indices):

        # Get the current timestamp
        timestamp = libraries.datetime.now().strftime("%Y-%m-%d_%H-%M")

        # Create the dynamic filename
        filename = f"test results_{timestamp}.pkl"

        results_df['Input Columns'] = input_cols
        results_df['Output Columns'] = output_cols
        results_df['Training Data Indices'] = training_indices
        results_df['Test Data Indices'] = test_indices

        # Save the DataFrame as a Pickle file
        results_df.to_pickle(filename)

        print(f"DataFrame saved to {filename}")

    def train_model(self, input_columns, output_columns):

        for cols in [input_columns, output_columns]:
            if 'index' in cols:
                cols.remove('index')

        sample_dict = self.sample_data()
        train_sample = sample_dict['train']
        test_sample = sample_dict['test']
        # establish training and testing data for GPARRegressor model
        # want to preserve these to be pandas DataFrames

        # adjust shape/data type of data sample to be ndarrays for GPAR
        training_input = WindFarmGPAR.specify_data(train_sample, input_columns)
        training_output = WindFarmGPAR.specify_data(train_sample, output_columns)
        test_input = WindFarmGPAR.specify_data(test_sample, input_columns)
        test_output = WindFarmGPAR.specify_data(test_sample, output_columns)

        # Fit and predict GPAR.
        model = libraries.gpar.GPARRegressor(
            scale=0.1,  # Initial length scale for the inputs.
            linear=True,  # Use linear dependencies between outputs.
            linear_scale=10.0,  # Length scale for linear dependencies between outputs.
            nonlinear=True,  # Also use nonlinear dependencies between outputs.
            nonlinear_scale=0.1,  # Length scale for nonlinear dependencies (post-normalisation, if enabled).
            noise=0.1,  # Variance of the observation noise.
            impute=True,  # Impute missing data to ensure data is closed downwards.
            replace=False,  # Do not replace data points with posterior mean of previous layer (retains noise).
            normalise_y=False  # Work with raw outputs, without normalising them.
        )

        model_df = pd.DataFrame(columns=['scale', 'linear', ...])
        model_df[1] = [1, False, 1000]

        dummy_model = libraries.gpar.GPARRegressor(
            scale=1,  # Initial length scale for the inputs.
            linear=False,  # Use linear dependencies between outputs.
            linear_scale=10.0,  # Length scale for linear dependencies between outputs.
            nonlinear=True,  # Also use nonlinear dependencies between outputs.
            nonlinear_scale=0.1,  # Length scale for nonlinear dependencies (post-normalisation, if enabled).
            noise=0.1,  # Variance of the observation noise.
            impute=True,  # Impute missing data to ensure data is closed downwards.
            replace=False,  # Do not replace data points with posterior mean of previous layer (retains noise).
            normalise_y=False  # Work with raw outputs, without normalising them.
        )

        dummy_df = pd.DataFrame(
            columns=['scale', 'linear', 'linear_scale', 'nonlinear', 'nonlinear_scale', 'noise', 'impute', 'replace',
                     'normalise_y'])

        dummy_df.loc[0] = [1, False, 10, True, 0.1, 0.1, True, False, False]
        print(dummy_df)

        dummy_df.to_pickle('dummy test pickle.pkl')

        # train model
        model.fit(training_input, training_output)

        # collect metadata
        means, lowers, uppers = model.predict(test_input,
                                              num_samples=5,
                                              credible_bounds=True)

        error = means - test_output

        metadata = {
            "Means": means.flatten(),
            "Lowers": lowers.flatten(),
            "Uppers": uppers.flatten(),
            "Error": error.flatten()
        }

        # organise and log results
        train_indices = train_sample[['index']].values.flatten()
        test_indices = test_sample[['index']].values.flatten()

        metadata = libraries.pd.DataFrame(metadata)

        WindFarmGPAR.log_results(
            results_df=metadata,
            input_cols=input_columns,
            output_cols=output_columns,
            training_indices=train_indices,
            test_indices=test_indices
        )


model_obj = WindFarmGPAR(train_data_path=libraries.os.getcwd() + "/Wind farm final year project _ SR_DL_PD/train.pkl",
                         test_data_path=libraries.os.getcwd() + "/Wind farm final year project _ SR_DL_PD/test.pkl",
                         local_path=libraries.os.getcwd())

model_obj.train_model(input_columns=['Wind.speed.me'],
                      output_columns=['Power.me'])

'''
# EXAMPLE FROM GITHUB (I adjusted the f_i functions and sample size for speed...!)

#if __name__ == "__main__":
wd = WorkingDirectory("test_run", seed = 1)

# Create toy data set.
n = 200
x = np.linspace(0, 1, n)  # generate n values between 0 and 1
noise = 0.1

# Draw functions depending on each other in complicated ways.
f1 = -np.clibraries.os(10 * np.pi * (x + 1)) / (2 * x + 1) - np.exp(x)
f2 = np.clibraries.os(f1) ** 2 + np.sin(x)
f3 = f2 * f1 ** 2 + 3 * x**5 - np.exp(f2)
f = np.stack((f1, f2, f3), axis=0).T  # combine f1, f2, f3 to a master np.array()

# Add noise and subsample.
y = f + noise * np.random.randn(n, 3)  # generate n x 3 samples from N(0,1) distribution,
                                        # scale them by our noise parameter, and element-wise add to f
x_obs, y_obs = x[::8], y[::8]  # not exactly a random sample, just takes every eighth element in x and y

# Fit and predict GPAR.
model = GPARRegressor(
    scale=0.1,              # Initial length scale for the inputs.
    linear=True,            # Use linear dependencies between outputs.
    linear_scale=10.0,      # Length scale for linear dependencies between outputs.
    nonlinear=True,         # Also use nonlinear dependencies between outputs.
    nonlinear_scale=0.1,    # Length scale for nonlinear dependencies (plibraries.ost-normalisation, if enabled).
    noise=0.1,              # Variance of the observation noise.
    impute=True,            # Impute missing data to ensure data is cllibraries.osed downwards.
    replace=False,          # Do not replace data points with plibraries.osterior mean of previous layer (may retain noise).
    normalise_y=False       # Work with raw outputs, without normalising them.
)

model.fit(x_obs, y_obs)
means, lowers, uppers = model.predict(
    x, num_samples=50, credible_bounds=True, latent=True
)

# Fit and predict independent GPs: set `markov=0` in GPAR.
igp = GPARRegressor(
    scale=0.1,
    linear=True,
    linear_scale=10.0,
    nonlinear=True,
    nonlinear_scale=0.1,
    noise=0.1,
    markov=0,
    normalise_y=False,
)
igp.fit(x_obs, y_obs)
igp_means, igp_lowers, igp_uppers = igp.predict(
    x, num_samples=50, credible_bounds=True, latent=True
)

# Plot the result.
plt.figure(figsize=(15, 3))

for i in range(3):
    plt.subplot(1, 3, i + 1)

    # Plot observations.
    plt.scatter(x_obs, y_obs[:, i], label="Observations", style="train")
    plt.plot(x, f[:, i], label="Truth", style="test")

    # Plot GPAR.
    plt.plot(x, means[:, i], label="GPAR", style="pred")
    plt.fill_between(x, lowers[:, i], uppers[:, i], style="pred")

    # Plot independent GPs.
    plt.plot(x, igp_means[:, i], label="IGP", style="pred2")
    plt.fill_between(x, igp_lowers[:, i], igp_uppers[:, i], style="pred2")

    plt.xlabel("$t$")
    plt.ylabel(f"$y_{i + 1}$")
    wbml.plot.tweak(legend=i == 2)

plt.tight_layout()
plt.savefig(wd.file("synthetic.pdf"))
'''
