# -*- coding: utf-8 -*-
"""STAT0035 Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EIyv_OmfhwVD_5KTqAZvXIKv1-a_vwvZ
"""

import libraries as libs


class WindFarmGPAR:
    __modelling_history_filepath = '/Users/sahmrahman/Library/CloudStorage/OneDrive-UniversityCollegeLondon/Year 3 UCL/STAT0035/GitHub/stat0035_project/Modelling History.pkl'
    __models_filepath = '/Users/sahmrahman/Library/CloudStorage/OneDrive-UniversityCollegeLondon/Year 3 UCL/STAT0035/GitHub/stat0035_project/Models.pkl'

    def __init__(self, train_data_path, test_data_path, model_params={}, existing=False, model_index=-1):
        """
        initialiser for a model

        :param train_data_path: filepath, string
        :param test_data_path: filepath, string
        :param local_path: filepath, string
        :param model_params: dictionary of 9 pairs, parameter name:value for the GPARRegressor model
        """
        self.train_data = libs.pd.read_pickle(train_data_path)
        self.test_data = libs.pd.read_pickle(test_data_path)

        # need to have some control over missing filepath

        self.model = WindFarmGPAR.create_model(existing, model_params, model_index)

        if model_index > -1:
            self.model_index = model_index
        else:
            self.model_index = len(libs.pkl.read_pickle_as_dataframe(WindFarmGPAR.__models_filepath)) - 1

    @staticmethod
    def create_model(existing, model_params, model_index):
        """
        create GPARRegressor model, either from scratch or rebuild

        :param existing: model exists already, bool
        :param model_params: model parameters, dictionary
        :param model_index: index in model pickle/dataframe, int
        :return: model, GPARRegressor instance
        """

        if existing and model_index > -1:
            # model exists, pull from pickle file @ given index

            models_df = libs.pkl.read_pickle_as_dataframe(WindFarmGPAR.__models_filepath)
            model_params = models_df.iloc[model_index]

            model_params = model_params[model_params.notna()]

            # model_params = model_params.where(model_params.notna(), None)
            # # replaces pandas `nan` values with None

        else:
            # need to make from scratch using model_params

            # add new model to our models file
            libs.pkl.append_to_pickle(WindFarmGPAR.__models_filepath, model_params)

        model = libs.gpar.GPARRegressor(**model_params)
        # ** is python's way of unpacking a dictionary as parameters

        # ================== GPARRegressor parameters ==================

        '''
        scale=0.1,            Initial length scale for the inputs.
        linear=True,          Use linear dependencies between outputs.
        linear_scale=10.0,    Length scale for linear dependencies between outputs.
        nonlinear=True,       Also use nonlinear dependencies between outputs.
        nonlinear_scale=0.1,  Length scale for nonlinear dependencies (post-normalisation, if enabled).
        noise=0.1,            Variance of the observation noise.
        impute=True,          Impute missing data to ensure data is closed downwards.
        replace=False,        Do not replace data points with posterior mean of previous layer (retains noise).
        normalise_y=False     Work with raw outputs, without normalising them.
        '''

        # there's some more, but I don't think I'll mess with those

        return model

    def sample_data(self):
        train_sample = self.train_data.iloc[:100]
        test_sample = self.test_data.iloc[:100]

        return {"train": train_sample,
                "test": test_sample}

    @staticmethod
    def specify_data(df, columns):
        """
        reshapes selected inputs to be agreeable with GPARs methods

        :param df: input values, pandas DataFrame
        :param columns: input columns, list of strings
        :return: 2D ndarray of selected columns from the dataframe, numpy ndarray
        """
        return df[columns].values

    @staticmethod
    def log_results(results_df, input_cols, output_cols, training_indices, test_indices, model_index):
        """
        log the results of a model run

        :param results_df: outputs, pandas DataFrame
        :param input_cols: input columns, list of strings
        :param output_cols: output columns, list of strings
        :param training_indices: indices of the original training data dataframe, list of ints
        :param test_indices: indices of the original test data dataframe, list of ints
        :param model_index: model index, int
        :return: NONE, simply saves results to pickle file and prints the filename
        """

        # Get the current timestamp
        timestamp = libs.datetime.now().strftime("%Y-%m-%d_%H-%M")

        results_df['Input Columns'] = [input_cols]
        results_df['Output Columns'] = [output_cols]
        results_df['Training Data Indices'] = [training_indices]
        results_df['Test Data Indices'] = [test_indices]
        # need to wrap the above in lists so their element in the dataframe is itself a list
        # (rather than multiple entries of the same column)

        results_df['Model Index'] = model_index
        results_df['Timestamp'] = timestamp

        # Save the DataFrame as a Pickle file

        libs.pkl.append_to_pickle(file_path=WindFarmGPAR.__modelling_history_filepath,
                                  new_row=results_df)

    def train_model(self, input_columns, output_columns):
        """
        1) sample data of requested input and output columns
        2) create the GPARRegressor model
        3) train the model
        4) store the results

        :param input_columns: input columns, list of strings
        :param output_columns: output columns, list of strings
        :return: NONE, trains model and logs the results
        """

        for cols in [input_columns, output_columns]:
            if 'index' in cols:
                cols.remove('index')

        sample_dict = self.sample_data()
        train_sample = sample_dict['train']
        test_sample = sample_dict['test']
        # establish training and testing data for GPARRegressor model
        # want to preserve these to be pandas DataFrames

        # adjust shape/data type of data sample to be ndarrays for GPAR
        training_input = WindFarmGPAR.specify_data(train_sample, input_columns)
        training_output = WindFarmGPAR.specify_data(train_sample, output_columns)
        test_input = WindFarmGPAR.specify_data(test_sample, input_columns)
        test_output = WindFarmGPAR.specify_data(test_sample, output_columns)

        # train model
        self.model.fit(training_input, training_output)

        # collect metadata
        means, lowers, uppers = self.model.predict(test_input,
                                                   num_samples=5,
                                                   credible_bounds=True)

        error = means - test_output

        metadata = {
            "Means": [means.flatten()],
            "Lowers": [lowers.flatten()],
            "Uppers": [uppers.flatten()],
            "Error": [error.flatten()]
        }

        # need to cast as whole list so lengths don't mess up

        # organise and log results
        train_indices = train_sample[['index']].values.flatten()
        test_indices = test_sample[['index']].values.flatten()

        metadata = libs.pd.DataFrame(metadata)

        WindFarmGPAR.log_results(
            results_df=metadata,
            input_cols=input_columns,
            output_cols=output_columns,
            training_indices=train_indices,
            test_indices=test_indices,
            model_index=self.model_index
        )


model_obj = WindFarmGPAR(
    train_data_path="/Users/sahmrahman/Library/CloudStorage/OneDrive-UniversityCollegeLondon/Year 3 UCL/STAT0035/Wind farm final year project _ SR_DL_PD/train.pkl",
    test_data_path="/Users/sahmrahman/Library/CloudStorage/OneDrive-UniversityCollegeLondon/Year 3 UCL/STAT0035/Wind farm final year project _ SR_DL_PD/test.pkl",
    model_params={'scale': 50.0})

# model_params={'scale': 0.1,
#               'linear': True,
#               'linear_scale': 10.0,
#               'nonlinear': True,
#               'nonlinear_scale': 0.1,
#               'noise': 0.1,
#               'impute': True,
#               'replace': False,
#               'normalise_y': False})

model_obj.train_model(input_columns=['Wind.speed.me'],
                      output_columns=['Power.me'])

# scale=0.1,            Initial length scale for the inputs.
# linear=True,          Use linear dependencies between outputs.
# linear_scale=10.0,    Length scale for linear dependencies between outputs.
# nonlinear=True,       Also use nonlinear dependencies between outputs.
# nonlinear_scale=0.1,  Length scale for nonlinear dependencies (post-normalisation, if enabled).
# noise=0.1,            Variance of the observation noise.
# impute=True,          Impute missing data to ensure data is closed downwards.
# replace=False,        Do not replace data points with posterior mean of previous layer (retains noise).
# normalise_y=False     Work with raw outputs, without normalising them.
